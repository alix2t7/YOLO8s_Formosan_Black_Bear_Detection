"""
GPU ç®¡ç†æ¨¡çµ„
æä¾›GPUæª¢æ¸¬ã€é…ç½®å’Œå„ªåŒ–åŠŸèƒ½
"""

import json
import os
import subprocess
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch


class GPUManager:
    """GPU ç®¡ç†å™¨"""

    def __init__(self):
        self.cuda_available = torch.cuda.is_available()
        self.gpu_count = torch.cuda.device_count() if self.cuda_available else 0
        self.gpu_info = self._get_gpu_info() if self.cuda_available else []

    def get_device(self) -> str:
        """ç²å–æ¨è–¦çš„è¨­å‚™å­—ç¬¦ä¸²"""
        if self.cuda_available and self.gpu_count > 0:
            # ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„GPU
            return "0"
        else:
            return "cpu"

    def _get_gpu_info(self) -> List[Dict[str, Any]]:
        """ç²å–GPUè©³ç´°ä¿¡æ¯"""
        gpu_info = []

        for i in range(self.gpu_count):
            try:
                props = torch.cuda.get_device_properties(i)
                info = {
                    "id": i,
                    "name": props.name,
                    "total_memory": props.total_memory,
                    "total_memory_gb": props.total_memory / 1024**3,
                    "major": props.major,
                    "minor": props.minor,
                    "multi_processor_count": props.multi_processor_count,
                    "available": True,
                }

                # ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
                torch.cuda.set_device(i)
                allocated = torch.cuda.memory_allocated(i)
                reserved = torch.cuda.memory_reserved(i)

                info.update(
                    {
                        "allocated_memory": allocated,
                        "allocated_memory_gb": allocated / 1024**3,
                        "reserved_memory": reserved,
                        "reserved_memory_gb": reserved / 1024**3,
                        "free_memory": props.total_memory - allocated,
                        "free_memory_gb": (props.total_memory - allocated) / 1024**3,
                        "utilization": allocated / props.total_memory * 100,
                    }
                )

                gpu_info.append(info)

            except Exception as e:
                gpu_info.append(
                    {"id": i, "name": "Unknown", "error": str(e), "available": False}
                )

        return gpu_info

    def get_nvidia_smi_info(self) -> Optional[Dict[str, Any]]:
        """é€šé nvidia-smi ç²å–GPUä¿¡æ¯"""
        try:
            result = subprocess.run(
                [
                    "nvidia-smi",
                    "--query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu",
                    "--format=csv,noheader,nounits",
                ],
                capture_output=True,
                text=True,
                check=True,
            )

            lines = result.stdout.strip().split("\n")
            smi_info = []

            for line in lines:
                parts = [part.strip() for part in line.split(",")]
                if len(parts) >= 6:
                    smi_info.append(
                        {
                            "index": int(parts[0]),
                            "name": parts[1],
                            "memory_total_mb": int(parts[2]),
                            "memory_used_mb": int(parts[3]),
                            "memory_free_mb": int(parts[4]),
                            "utilization_percent": int(parts[5]),
                        }
                    )

            return {"gpus": smi_info, "available": True}

        except (subprocess.CalledProcessError, FileNotFoundError, ValueError):
            return None

    def print_gpu_info(self):
        """æ‰“å°GPUä¿¡æ¯"""
        print("=== GPU ä¿¡æ¯ ===")

        if not self.cuda_available:
            print("âŒ CUDA ä¸å¯ç”¨")
            return

        print(f"âœ… CUDA å¯ç”¨ï¼Œæª¢æ¸¬åˆ° {self.gpu_count} å€‹GPU")

        for gpu in self.gpu_info:
            if gpu.get("available", False):
                print(f"\nğŸ”¹ GPU {gpu['id']}: {gpu['name']}")
                print(f"   ç¸½è¨˜æ†¶é«”: {gpu['total_memory_gb']:.1f} GB")
                print(f"   å·²åˆ†é…: {gpu['allocated_memory_gb']:.2f} GB")
                print(f"   å·²ä¿ç•™: {gpu['reserved_memory_gb']:.2f} GB")
                print(f"   å¯ç”¨: {gpu['free_memory_gb']:.2f} GB")
                print(f"   ä½¿ç”¨ç‡: {gpu['utilization']:.1f}%")
                print(f"   è¨ˆç®—èƒ½åŠ›: {gpu['major']}.{gpu['minor']}")
                print(f"   å¤šè™•ç†å™¨: {gpu['multi_processor_count']}")
            else:
                print(f"\nâŒ GPU {gpu['id']}: ä¸å¯ç”¨")
                if "error" in gpu:
                    print(f"   éŒ¯èª¤: {gpu['error']}")

        # nvidia-smi ä¿¡æ¯
        smi_info = self.get_nvidia_smi_info()
        if smi_info:
            print(f"\nğŸ“Š nvidia-smi å³æ™‚ç‹€æ…‹:")
            for gpu in smi_info["gpus"]:
                print(
                    f"   GPU {gpu['index']}: {gpu['utilization_percent']}% ä½¿ç”¨ç‡, "
                    f"{gpu['memory_used_mb']}/{gpu['memory_total_mb']} MB è¨˜æ†¶é«”"
                )

    def get_optimal_device_setup(
        self, preferred_gpus: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """ç²å–æœ€ä½³è¨­å‚™é…ç½®"""
        config = {
            "use_multi_gpu": False,
            "device_ids": [0],
            "strategy": "cpu",
            "available_memory_gb": 0,
            "recommended_batch_multiplier": 1,
        }

        if not self.cuda_available or self.gpu_count == 0:
            config["strategy"] = "cpu"
            return config

        # ç¯©é¸å¯ç”¨GPU
        available_gpus = [gpu for gpu in self.gpu_info if gpu.get("available", False)]

        if not available_gpus:
            config["strategy"] = "cpu"
            return config

        # å¦‚æœæŒ‡å®šäº†å„ªå…ˆGPUï¼Œå…ˆæª¢æŸ¥å¯ç”¨æ€§
        if preferred_gpus:
            valid_preferred = [
                gpu for gpu in available_gpus if gpu["id"] in preferred_gpus
            ]
            if valid_preferred:
                available_gpus = valid_preferred

        # æŒ‰è¨˜æ†¶é«”å¤§å°æ’åº
        available_gpus.sort(key=lambda x: x["free_memory_gb"], reverse=True)

        # å–®GPUé…ç½®
        best_gpu = available_gpus[0]
        config.update(
            {
                "strategy": "single-gpu",
                "device_ids": [best_gpu["id"]],
                "available_memory_gb": best_gpu["free_memory_gb"],
                "recommended_batch_multiplier": 1,
            }
        )

        # å¤šGPUé…ç½®æª¢æŸ¥
        if len(available_gpus) >= 2:
            # æª¢æŸ¥å‰å…©å€‹GPUæ˜¯å¦è¨˜æ†¶é«”ç›¸è¿‘
            gpu1, gpu2 = available_gpus[0], available_gpus[1]
            memory_diff = abs(gpu1["total_memory_gb"] - gpu2["total_memory_gb"])

            if memory_diff < 2.0:  # è¨˜æ†¶é«”å·®ç•°å°æ–¼2GB
                config.update(
                    {
                        "use_multi_gpu": True,
                        "strategy": "multi-gpu",
                        "device_ids": [gpu1["id"], gpu2["id"]],
                        "available_memory_gb": min(
                            gpu1["free_memory_gb"], gpu2["free_memory_gb"]
                        ),
                        "recommended_batch_multiplier": 2,
                    }
                )

        return config

    def setup_cuda_environment(self, device_ids: List[int]) -> bool:
        """è¨­ç½®CUDAç’°å¢ƒ"""
        if not self.cuda_available:
            return False

        try:
            # è¨­ç½®å¯è¦‹GPU
            os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, device_ids))

            # é ç†±GPU
            for device_id in device_ids:
                if device_id < self.gpu_count:
                    torch.cuda.set_device(device_id)
                    # ç°¡å–®çš„è¨˜æ†¶é«”æ¸¬è©¦
                    test_tensor = torch.rand(100, 100).cuda()
                    del test_tensor
                    torch.cuda.empty_cache()

            return True

        except Exception as e:
            print(f"âŒ CUDAç’°å¢ƒè¨­ç½®å¤±æ•—: {e}")
            return False

    def clear_gpu_memory(self, device_ids: Optional[List[int]] = None):
        """æ¸…ç†GPUè¨˜æ†¶é«”"""
        if not self.cuda_available:
            return

        if device_ids is None:
            device_ids = list(range(self.gpu_count))

        for device_id in device_ids:
            try:
                torch.cuda.set_device(device_id)
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"âš ï¸  æ¸…ç†GPU {device_id} è¨˜æ†¶é«”å¤±æ•—: {e}")

    def monitor_gpu_usage(self) -> Dict[str, Any]:
        """ç›£æ§GPUä½¿ç”¨æƒ…æ³"""
        if not self.cuda_available:
            return {"available": False}

        usage_info = {"available": True, "gpus": []}

        for i in range(self.gpu_count):
            try:
                torch.cuda.set_device(i)
                allocated = torch.cuda.memory_allocated(i)
                reserved = torch.cuda.memory_reserved(i)
                total = torch.cuda.get_device_properties(i).total_memory

                gpu_usage = {
                    "id": i,
                    "allocated_gb": allocated / 1024**3,
                    "reserved_gb": reserved / 1024**3,
                    "total_gb": total / 1024**3,
                    "free_gb": (total - allocated) / 1024**3,
                    "utilization_percent": allocated / total * 100,
                }

                usage_info["gpus"].append(gpu_usage)

            except Exception as e:
                usage_info["gpus"].append({"id": i, "error": str(e)})

        return usage_info

    def get_recommended_batch_size(
        self, model_size: str, img_size: int, device_ids: List[int]
    ) -> int:
        """æ ¹æ“šGPUé…ç½®æ¨è–¦æ‰¹æ¬¡å¤§å°"""
        if not self.cuda_available:
            return 4  # CPUæ¨¡å¼ä½¿ç”¨å°æ‰¹æ¬¡

        # åŸºç¤æ‰¹æ¬¡å¤§å°é…ç½®
        base_batch_sizes = {
            "yolov8n": {"640": 128, "512": 192, "320": 256},
            "yolov8s": {"640": 64, "512": 96, "320": 128},
            "yolov8m": {"640": 32, "512": 48, "320": 64},
            "yolov8l": {"640": 16, "512": 24, "320": 32},
            "yolov8x": {"640": 8, "512": 12, "320": 16},
        }

        # ç²å–åŸºç¤æ‰¹æ¬¡å¤§å°
        base_batch = base_batch_sizes.get(model_size, {}).get(str(img_size), 32)

        # æ ¹æ“šGPUæ•¸é‡èª¿æ•´
        gpu_count = len(device_ids)

        if gpu_count >= 2:
            # å¤šGPUå¯ä»¥ä½¿ç”¨æ›´å¤§æ‰¹æ¬¡
            return base_batch * gpu_count
        else:
            # å–®GPUï¼Œæª¢æŸ¥è¨˜æ†¶é«”
            if device_ids[0] < len(self.gpu_info):
                gpu = self.gpu_info[device_ids[0]]
                memory_gb = gpu.get("free_memory_gb", 8)

                if memory_gb >= 12:
                    return base_batch
                elif memory_gb >= 8:
                    return base_batch // 2
                else:
                    return max(base_batch // 4, 1)

            return base_batch

    def validate_configuration(self, config: Dict[str, Any]) -> Tuple[bool, str]:
        """é©—è­‰GPUé…ç½®"""
        if not config.get("use_multi_gpu", False):
            return True, "å–®GPUé…ç½®æœ‰æ•ˆ"

        device_ids = config.get("device_ids", [])

        if len(device_ids) < 2:
            return False, "å¤šGPUæ¨¡å¼éœ€è¦è‡³å°‘2å€‹GPU"

        for device_id in device_ids:
            if device_id >= self.gpu_count:
                return False, f"GPU {device_id} ä¸å­˜åœ¨"

            if not self.gpu_info[device_id].get("available", False):
                return False, f"GPU {device_id} ä¸å¯ç”¨"

        return True, "å¤šGPUé…ç½®æœ‰æ•ˆ"

    def save_gpu_report(self, filepath: Path):
        """ä¿å­˜GPUå ±å‘Š"""
        from datetime import datetime

        report = {
            "timestamp": datetime.now().isoformat(),
            "cuda_available": self.cuda_available,
            "gpu_count": self.gpu_count,
            "gpu_info": self.gpu_info,
            "nvidia_smi": self.get_nvidia_smi_info(),
            "optimal_config": self.get_optimal_device_setup(),
        }

        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)


# å…¨åŸŸGPUç®¡ç†å™¨å¯¦ä¾‹
_gpu_manager: Optional[GPUManager] = None


def get_gpu_manager() -> GPUManager:
    """ç²å–GPUç®¡ç†å™¨å¯¦ä¾‹"""
    global _gpu_manager
    if _gpu_manager is None:
        _gpu_manager = GPUManager()
    return _gpu_manager
