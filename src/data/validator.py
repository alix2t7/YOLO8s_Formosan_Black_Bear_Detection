"""
æ•¸æ“šé©—è­‰å™¨
"""

import os
import yaml
import cv2
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from pathlib import Path
import json

class DataValidator:
    """æ•¸æ“šé©—è­‰å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.errors = []
        self.warnings = []
        
    def validate_complete_dataset(self, dataset_path: str) -> Dict[str, Any]:
        """å®Œæ•´æ•¸æ“šé›†é©—è­‰"""
        results = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'statistics': {},
            'recommendations': []
        }
        
        try:
            # 1. çµæ§‹é©—è­‰
            structure_valid, structure_errors = self._validate_structure(dataset_path)
            if not structure_valid:
                results['errors'].extend(structure_errors)
                results['is_valid'] = False
            
            # 2. é…ç½®æ–‡ä»¶é©—è­‰
            config_valid, config_errors = self._validate_config_file(dataset_path)
            if not config_valid:
                results['errors'].extend(config_errors)
                results['is_valid'] = False
            
            # 3. æ•¸æ“šä¸€è‡´æ€§é©—è­‰
            consistency_valid, consistency_errors, consistency_warnings = self._validate_data_consistency(dataset_path)
            if not consistency_valid:
                results['errors'].extend(consistency_errors)
                results['is_valid'] = False
            results['warnings'].extend(consistency_warnings)
            
            # 4. åœ–åƒè³ªé‡é©—è­‰
            quality_warnings = self._validate_image_quality(dataset_path)
            results['warnings'].extend(quality_warnings)
            
            # 5. æ¨™ç±¤æ ¼å¼é©—è­‰
            label_valid, label_errors, label_warnings = self._validate_labels(dataset_path)
            if not label_valid:
                results['errors'].extend(label_errors)
                results['is_valid'] = False
            results['warnings'].extend(label_warnings)
            
            # 6. ç”Ÿæˆçµ±è¨ˆä¿¡æ¯
            results['statistics'] = self._generate_statistics(dataset_path)
            
            # 7. ç”Ÿæˆå»ºè­°
            results['recommendations'] = self._generate_recommendations(results)
            
        except Exception as e:
            results['errors'].append(f"é©—è­‰éç¨‹å‡ºéŒ¯: {str(e)}")
            results['is_valid'] = False
        
        return results
    
    def _validate_structure(self, dataset_path: str) -> Tuple[bool, List[str]]:
        """é©—è­‰ç›®éŒ„çµæ§‹"""
        errors = []
        
        # æª¢æŸ¥ä¸»ç›®éŒ„
        if not os.path.exists(dataset_path):
            errors.append(f"æ•¸æ“šé›†ç›®éŒ„ä¸å­˜åœ¨: {dataset_path}")
            return False, errors
        
        # æª¢æŸ¥å¿…éœ€çš„å­ç›®éŒ„
        required_dirs = [
            'images/train',
            'images/val', 
            'labels/train',
            'labels/val'
        ]
        
        for dir_path in required_dirs:
            full_path = os.path.join(dataset_path, dir_path)
            if not os.path.exists(full_path):
                errors.append(f"å¿…éœ€ç›®éŒ„ä¸å­˜åœ¨: {full_path}")
        
        return len(errors) == 0, errors
    
    def _validate_config_file(self, dataset_path: str) -> Tuple[bool, List[str]]:
        """é©—è­‰é…ç½®æ–‡ä»¶"""
        errors = []
        
        # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
        config_files = []
        for file in os.listdir(dataset_path):
            if file.endswith(('.yaml', '.yml')) and 'data' in file.lower():
                config_files.append(os.path.join(dataset_path, file))
        
        if not config_files:
            errors.append("æœªæ‰¾åˆ°æ•¸æ“šé›†é…ç½®æ–‡ä»¶ (.yaml/.yml)")
            return False, errors
        
        # é©—è­‰é…ç½®æ–‡ä»¶å…§å®¹
        for config_file in config_files:
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                # æª¢æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['train', 'val', 'nc', 'names']
                for field in required_fields:
                    if field not in config:
                        errors.append(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å­—æ®µ '{field}': {config_file}")
                
                # æª¢æŸ¥é¡åˆ¥æ•¸é‡ä¸€è‡´æ€§
                if 'nc' in config and 'names' in config:
                    if config['nc'] != len(config['names']):
                        errors.append(f"é¡åˆ¥æ•¸é‡ä¸ä¸€è‡´: nc={config['nc']}, namesæ•¸é‡={len(config['names'])}")
                
                # æª¢æŸ¥è·¯å¾‘
                for path_key in ['train', 'val']:
                    if path_key in config:
                        path = config[path_key]
                        if not os.path.isabs(path):
                            # ç›¸å°è·¯å¾‘ï¼Œç›¸å°æ–¼é…ç½®æ–‡ä»¶ç›®éŒ„
                            path = os.path.join(os.path.dirname(config_file), path)
                        if not os.path.exists(path):
                            errors.append(f"é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾‘ä¸å­˜åœ¨: {path}")
                
            except Exception as e:
                errors.append(f"é…ç½®æ–‡ä»¶è§£æå¤±æ•—: {config_file}, éŒ¯èª¤: {str(e)}")
        
        return len(errors) == 0, errors
    
    def _validate_data_consistency(self, dataset_path: str) -> Tuple[bool, List[str], List[str]]:
        """é©—è­‰æ•¸æ“šä¸€è‡´æ€§"""
        errors = []
        warnings = []
        
        for split in ['train', 'val']:
            images_dir = os.path.join(dataset_path, f'images/{split}')
            labels_dir = os.path.join(dataset_path, f'labels/{split}')
            
            if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
                continue
            
            # ç²å–åœ–åƒå’Œæ¨™ç±¤æ–‡ä»¶
            image_files = {os.path.splitext(f)[0] for f in os.listdir(images_dir) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))}
            label_files = {os.path.splitext(f)[0] for f in os.listdir(labels_dir) 
                          if f.endswith('.txt')}
            
            # æª¢æŸ¥ç¼ºå¤±çš„æ¨™ç±¤æ–‡ä»¶
            missing_labels = image_files - label_files
            if missing_labels:
                if len(missing_labels) > 10:
                    errors.append(f"{split}: {len(missing_labels)} å€‹åœ–åƒç¼ºå°‘æ¨™ç±¤æ–‡ä»¶")
                else:
                    for name in list(missing_labels)[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        warnings.append(f"{split}: åœ–åƒ {name} ç¼ºå°‘æ¨™ç±¤æ–‡ä»¶")
            
            # æª¢æŸ¥å¤šé¤˜çš„æ¨™ç±¤æ–‡ä»¶
            extra_labels = label_files - image_files
            if extra_labels:
                if len(extra_labels) > 10:
                    warnings.append(f"{split}: {len(extra_labels)} å€‹æ¨™ç±¤æ–‡ä»¶æ²’æœ‰å°æ‡‰åœ–åƒ")
                else:
                    for name in list(extra_labels)[:5]:
                        warnings.append(f"{split}: æ¨™ç±¤ {name} æ²’æœ‰å°æ‡‰åœ–åƒ")
        
        return len(errors) == 0, errors, warnings
    
    def _validate_image_quality(self, dataset_path: str) -> List[str]:
        """é©—è­‰åœ–åƒè³ªé‡"""
        warnings = []
        
        for split in ['train', 'val']:
            images_dir = os.path.join(dataset_path, f'images/{split}')
            if not os.path.exists(images_dir):
                continue
            
            image_files = [f for f in os.listdir(images_dir) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            
            # æª¢æŸ¥å°‘é‡åœ–åƒä½œç‚ºæ¨£æœ¬
            sample_size = min(20, len(image_files))
            sample_files = image_files[:sample_size]
            
            corrupted_count = 0
            small_images = 0
            large_images = 0
            
            for image_file in sample_files:
                image_path = os.path.join(images_dir, image_file)
                
                try:
                    # æª¢æŸ¥åœ–åƒæ˜¯å¦å¯è®€
                    img = cv2.imread(image_path)
                    if img is None:
                        corrupted_count += 1
                        continue
                    
                    height, width = img.shape[:2]
                    
                    # æª¢æŸ¥åœ–åƒå°ºå¯¸
                    if width < 32 or height < 32:
                        small_images += 1
                    elif width > 4096 or height > 4096:
                        large_images += 1
                    
                except Exception:
                    corrupted_count += 1
            
            # ç”Ÿæˆè­¦å‘Š
            if corrupted_count > 0:
                warnings.append(f"{split}: ç™¼ç¾ {corrupted_count} å€‹æå£çš„åœ–åƒï¼ˆæ¨£æœ¬æª¢æŸ¥ï¼‰")
            
            if small_images > 0:
                warnings.append(f"{split}: ç™¼ç¾ {small_images} å€‹éå°çš„åœ–åƒï¼ˆ<32pxï¼‰")
            
            if large_images > 0:
                warnings.append(f"{split}: ç™¼ç¾ {large_images} å€‹éå¤§çš„åœ–åƒï¼ˆ>4096pxï¼‰")
        
        return warnings
    
    def _validate_labels(self, dataset_path: str) -> Tuple[bool, List[str], List[str]]:
        """é©—è­‰æ¨™ç±¤æ ¼å¼"""
        errors = []
        warnings = []
        
        for split in ['train', 'val']:
            labels_dir = os.path.join(dataset_path, f'labels/{split}')
            if not os.path.exists(labels_dir):
                continue
            
            label_files = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]
            
            # æª¢æŸ¥éƒ¨åˆ†æ¨™ç±¤æ–‡ä»¶
            sample_size = min(50, len(label_files))
            sample_files = label_files[:sample_size]
            
            invalid_format_count = 0
            invalid_values_count = 0
            empty_files_count = 0
            
            for label_file in sample_files:
                label_path = os.path.join(labels_dir, label_file)
                
                try:
                    with open(label_path, 'r') as f:
                        lines = f.readlines()
                    
                    if not lines:
                        empty_files_count += 1
                        continue
                    
                    for line_num, line in enumerate(lines, 1):
                        line = line.strip()
                        if not line:
                            continue
                        
                        parts = line.split()
                        
                        # æª¢æŸ¥æ ¼å¼ï¼šclass_id x_center y_center width height
                        if len(parts) != 5:
                            invalid_format_count += 1
                            break
                        
                        try:
                            class_id = int(parts[0])
                            x_center, y_center, width, height = map(float, parts[1:])
                            
                            # æª¢æŸ¥å€¼ç¯„åœ
                            if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 
                                   0 <= width <= 1 and 0 <= height <= 1):
                                invalid_values_count += 1
                                break
                            
                            # æª¢æŸ¥é¡åˆ¥IDï¼ˆå‡è¨­æ˜¯ç†Šé¡æª¢æ¸¬ï¼š0=kumay, 1=not_kumayï¼‰
                            if class_id not in [0, 1]:
                                warnings.append(f"{split}: æœªçŸ¥é¡åˆ¥ID {class_id} åœ¨ {label_file}:{line_num}")
                        
                        except ValueError:
                            invalid_format_count += 1
                            break
                
                except Exception:
                    invalid_format_count += 1
            
            # ç”ŸæˆéŒ¯èª¤å’Œè­¦å‘Š
            if invalid_format_count > 0:
                errors.append(f"{split}: {invalid_format_count} å€‹æ¨™ç±¤æ–‡ä»¶æ ¼å¼éŒ¯èª¤ï¼ˆæ¨£æœ¬æª¢æŸ¥ï¼‰")
            
            if invalid_values_count > 0:
                errors.append(f"{split}: {invalid_values_count} å€‹æ¨™ç±¤æ–‡ä»¶æ•¸å€¼è¶…å‡ºç¯„åœï¼ˆæ¨£æœ¬æª¢æŸ¥ï¼‰")
            
            if empty_files_count > 0:
                warnings.append(f"{split}: {empty_files_count} å€‹ç©ºæ¨™ç±¤æ–‡ä»¶")
        
        return len(errors) == 0, errors, warnings
    
    def _generate_statistics(self, dataset_path: str) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±è¨ˆä¿¡æ¯"""
        stats = {
            'image_counts': {},
            'label_distribution': {},
            'file_sizes': {},
            'class_balance': {}
        }
        
        try:
            total_kumay = 0
            total_not_kumay = 0
            
            for split in ['train', 'val']:
                # åœ–åƒçµ±è¨ˆ
                images_dir = os.path.join(dataset_path, f'images/{split}')
                if os.path.exists(images_dir):
                    image_files = [f for f in os.listdir(images_dir) 
                                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                    stats['image_counts'][split] = len(image_files)
                else:
                    stats['image_counts'][split] = 0
                
                # æ¨™ç±¤çµ±è¨ˆ
                labels_dir = os.path.join(dataset_path, f'labels/{split}')
                if os.path.exists(labels_dir):
                    kumay_count = 0
                    not_kumay_count = 0
                    
                    for label_file in os.listdir(labels_dir):
                        if not label_file.endswith('.txt'):
                            continue
                        
                        label_path = os.path.join(labels_dir, label_file)
                        try:
                            with open(label_path, 'r') as f:
                                lines = f.readlines()
                            
                            for line in lines:
                                if line.strip():
                                    class_id = int(line.split()[0])
                                    if class_id == 0:
                                        kumay_count += 1
                                    elif class_id == 1:
                                        not_kumay_count += 1
                        except:
                            continue
                    
                    stats['label_distribution'][split] = {
                        'kumay': kumay_count,
                        'not_kumay': not_kumay_count
                    }
                    
                    total_kumay += kumay_count
                    total_not_kumay += not_kumay_count
            
            # é¡åˆ¥å¹³è¡¡
            total_labels = total_kumay + total_not_kumay
            if total_labels > 0:
                stats['class_balance'] = {
                    'kumay_ratio': total_kumay / total_labels,
                    'not_kumay_ratio': total_not_kumay / total_labels,
                    'balance_score': min(total_kumay, total_not_kumay) / max(total_kumay, total_not_kumay) if max(total_kumay, total_not_kumay) > 0 else 0
                }
        
        except Exception as e:
            stats['error'] = f"çµ±è¨ˆç”Ÿæˆå¤±æ•—: {str(e)}"
        
        return stats
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼éŒ¯èª¤ç”Ÿæˆå»ºè­°
        if results['errors']:
            recommendations.append("ğŸ”´ ä¿®å¾©æ‰€æœ‰éŒ¯èª¤å¾Œå†é–‹å§‹è¨“ç·´")
        
        # åŸºæ–¼çµ±è¨ˆç”Ÿæˆå»ºè­°
        stats = results.get('statistics', {})
        
        # æª¢æŸ¥æ•¸æ“šé‡
        image_counts = stats.get('image_counts', {})
        train_count = image_counts.get('train', 0)
        val_count = image_counts.get('val', 0)
        
        if train_count < 100:
            recommendations.append("âš ï¸  è¨“ç·´åœ–åƒæ•¸é‡è¼ƒå°‘ï¼Œå»ºè­°å¢åŠ æ•¸æ“šæˆ–ä½¿ç”¨æ•¸æ“šå¢å¼·")
        
        if val_count < 20:
            recommendations.append("âš ï¸  é©—è­‰åœ–åƒæ•¸é‡è¼ƒå°‘ï¼Œå»ºè­°å¢åŠ é©—è­‰æ•¸æ“š")
        
        # æª¢æŸ¥é¡åˆ¥å¹³è¡¡
        class_balance = stats.get('class_balance', {})
        balance_score = class_balance.get('balance_score', 1.0)
        
        if balance_score < 0.3:
            recommendations.append("âš ï¸  é¡åˆ¥ä¸å¹³è¡¡åš´é‡ï¼Œå»ºè­°ä½¿ç”¨é¡åˆ¥æ¬Šé‡æˆ–é‡æ¡æ¨£")
        elif balance_score < 0.5:
            recommendations.append("ğŸ’¡ é¡åˆ¥ç•¥æœ‰ä¸å¹³è¡¡ï¼Œå¯è€ƒæ…®èª¿æ•´é¡åˆ¥æ¬Šé‡")
        
        # åŸºæ–¼è­¦å‘Šç”Ÿæˆå»ºè­°
        if any("ç©ºæ¨™ç±¤æ–‡ä»¶" in w for w in results['warnings']):
            recommendations.append("ğŸ’¡ æ¸…ç†ç©ºæ¨™ç±¤æ–‡ä»¶æˆ–ç¢ºèªæ˜¯å¦ç‚ºè² æ¨£æœ¬")
        
        if any("æå£" in w for w in results['warnings']):
            recommendations.append("ğŸ”§ ä¿®å¾©æˆ–ç§»é™¤æå£çš„åœ–åƒæ–‡ä»¶")
        
        if not recommendations:
            recommendations.append("âœ… æ•¸æ“šé›†è³ªé‡è‰¯å¥½ï¼Œå¯ä»¥é–‹å§‹è¨“ç·´")
        
        return recommendations
    
    def save_validation_report(self, results: Dict[str, Any], output_path: str) -> None:
        """ä¿å­˜é©—è­‰å ±å‘Š"""
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… é©—è­‰å ±å‘Šå·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜é©—è­‰å ±å‘Šå¤±æ•—: {str(e)}")
